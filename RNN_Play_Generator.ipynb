{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s92qrKCvIW1",
        "outputId": "d4a48b1a-9768-44f0-ec6c-a18b13212c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IourtjxxvhsC",
        "outputId": "2c350315-f83a-4a34-8580-1cfc2689f9b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 1s 1us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIcHaxipvliG"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "#path_to_file = list(files.upload().keys())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8L74MPLvvfp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e76fd58f-636e-4889-a59e-2dcddee6ee94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCXVuKE-wRRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af200699-e437-4827-a2fa-b68db2b3b9f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HtIejIhwR7I"
      },
      "outputs": [],
      "source": [
        "vocab = sorted(set(text))\n",
        "#print(vocab)\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFjFRCwVwSHU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a88b2bf-72d4-4c5d-dd4d-eea1895b366c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ],
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    #print(ints)\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N--FvyHf721E"
      },
      "outputs": [],
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LbgWKfg75IZ"
      },
      "outputs": [],
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42fD6-Fa9mzA"
      },
      "outputs": [],
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0TAeUkw97um",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "153ed820-3f02-4fcd-9274-6d26b0ad50df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ],
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiNgRgqe-x67"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_fqT0yNEh1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e1c74e9-2390-4309-d9c6-370ad2523f1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5330241 (20.33 MB)\n",
            "Trainable params: 5330241 (20.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suOTgjDtEuUl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f45f42-75ec-4496-9852-be73f6d1a06e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWmS37U-Ew0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "701336fb-5aca-486c-f06a-47b13847cdf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-4.6815919e-03  3.7439226e-03  4.8695426e-03 ...  2.7518282e-03\n",
            "   -1.2373817e-03 -5.1891957e-03]\n",
            "  [-7.1116928e-03  2.4388877e-03 -2.7195443e-03 ... -1.6120905e-03\n",
            "   -3.2213735e-03 -1.6790213e-03]\n",
            "  [-3.8629407e-03  2.2983672e-03 -2.0372362e-03 ... -2.4623112e-03\n",
            "   -9.6476823e-04  9.2965353e-04]\n",
            "  ...\n",
            "  [-1.2632029e-02 -1.5756049e-03  8.3591361e-03 ...  7.2647505e-03\n",
            "    2.5735884e-03 -4.4383113e-03]\n",
            "  [-8.5563501e-03 -1.0930412e-03  6.8003647e-03 ...  4.7730552e-03\n",
            "    3.4861271e-03 -1.8534412e-03]\n",
            "  [-7.7565745e-03 -1.3080130e-02 -1.9308982e-03 ...  7.4420981e-03\n",
            "    5.6326180e-03 -5.2800491e-03]]\n",
            "\n",
            " [[ 5.9712012e-03  4.5018592e-03  1.4695895e-03 ...  2.2031851e-03\n",
            "   -4.6121422e-03  3.6729949e-03]\n",
            "  [ 1.2774410e-04  8.4255831e-03  6.4125750e-03 ...  1.9652119e-03\n",
            "   -1.2341477e-03  5.0920630e-03]\n",
            "  [-3.2079010e-04  5.6550996e-03  1.0030738e-02 ...  3.7912349e-03\n",
            "    2.4715397e-03  7.1697019e-04]\n",
            "  ...\n",
            "  [-7.2304429e-03 -4.1282098e-03  1.4459397e-02 ... -5.6593679e-04\n",
            "    1.2540370e-03  6.4770253e-03]\n",
            "  [-6.1867721e-03 -4.2874683e-03  1.7635081e-02 ...  3.3593182e-03\n",
            "    5.4809176e-03  9.4851926e-03]\n",
            "  [-1.0450776e-02  7.5093261e-04  1.9941799e-02 ...  4.3785311e-03\n",
            "    4.5737932e-03  5.3245260e-04]]\n",
            "\n",
            " [[-4.2421650e-03  9.1906020e-04  2.9149954e-04 ...  4.9389079e-03\n",
            "   -5.1566083e-03 -1.4309483e-03]\n",
            "  [-2.7586734e-03  3.4294743e-03  2.0238988e-03 ...  2.0933216e-03\n",
            "   -8.9389766e-03  1.7018561e-03]\n",
            "  [-2.1611033e-03  5.1418063e-03  6.4671347e-03 ...  2.2067663e-03\n",
            "   -1.0797965e-02 -4.5615835e-03]\n",
            "  ...\n",
            "  [-9.3197562e-03  5.3771036e-03  4.2742803e-03 ...  8.3708717e-04\n",
            "   -5.2237520e-03  1.1452604e-03]\n",
            "  [-7.3063960e-03  3.8727163e-03  9.0359580e-03 ...  5.7002720e-03\n",
            "    1.3031859e-03  5.8130333e-03]\n",
            "  [-7.5915973e-03  8.8029280e-03  3.7714224e-03 ...  1.9327425e-03\n",
            "    5.1401844e-03  4.5242081e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.2779823e-03 -6.0254260e-04  5.8349641e-03 ...  2.6916722e-03\n",
            "    3.8033023e-03 -3.4374045e-03]\n",
            "  [ 2.7089973e-04 -1.3330785e-04  4.6097306e-03 ...  5.8163761e-04\n",
            "    4.3625534e-03 -1.2330771e-03]\n",
            "  [-3.0098925e-04 -2.5074421e-03 -2.5257911e-03 ...  1.9976781e-03\n",
            "   -2.7674749e-03  4.3180692e-03]\n",
            "  ...\n",
            "  [ 8.1301918e-03  9.5766988e-03  6.9331415e-03 ...  2.3778700e-03\n",
            "   -3.1369096e-03 -1.3743336e-03]\n",
            "  [ 5.0523393e-03  6.9690887e-03  1.0179574e-02 ...  4.6296520e-03\n",
            "    2.2193375e-03 -6.6102045e-03]\n",
            "  [ 2.4218026e-03 -5.7242927e-05  1.0112094e-02 ...  8.8885622e-03\n",
            "    2.8138403e-03 -4.9869879e-03]]\n",
            "\n",
            " [[ 1.2740714e-03  5.1433581e-04  8.5646636e-05 ... -2.0466747e-03\n",
            "    1.5466909e-03  1.5414258e-03]\n",
            "  [ 3.1589908e-03 -4.5717116e-03  8.8182045e-04 ... -4.2234408e-03\n",
            "    2.8834939e-03 -4.2633107e-03]\n",
            "  [ 1.1007795e-03  9.4870466e-04 -3.8125510e-03 ... -5.7718763e-03\n",
            "    4.5305002e-03 -3.6803377e-03]\n",
            "  ...\n",
            "  [-9.7303819e-03 -1.0768254e-02  1.6472705e-02 ...  1.6059084e-02\n",
            "    1.0513571e-02 -1.0437126e-02]\n",
            "  [-1.2591385e-02 -2.6281329e-03  1.8176472e-02 ...  2.0765211e-02\n",
            "    9.6241310e-03 -1.2087371e-02]\n",
            "  [-1.1930374e-02  1.5963693e-03  9.7285807e-03 ...  1.3496147e-02\n",
            "    8.8061271e-03 -7.4619632e-03]]\n",
            "\n",
            " [[-1.2779823e-03 -6.0254260e-04  5.8349641e-03 ...  2.6916722e-03\n",
            "    3.8033023e-03 -3.4374045e-03]\n",
            "  [-6.9566828e-05 -5.8809221e-03  6.0316310e-03 ...  2.7488687e-03\n",
            "   -4.8760627e-04 -7.1700485e-03]\n",
            "  [ 2.9864735e-03 -1.0634072e-02  7.9575786e-03 ...  1.3510393e-03\n",
            "    6.9220527e-04 -1.0719910e-02]\n",
            "  ...\n",
            "  [-6.7496328e-03 -4.2919563e-03  1.0879054e-02 ...  4.1966350e-03\n",
            "    1.7939215e-03 -1.1018107e-02]\n",
            "  [-7.9994202e-03 -1.4837581e-03  1.0594244e-02 ...  8.9815147e-03\n",
            "   -4.2404430e-03 -1.0527670e-02]\n",
            "  [-8.9390837e-03 -1.8191379e-03  1.8726835e-02 ...  4.4021909e-03\n",
            "   -2.1080512e-03 -9.1373641e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCpzXW1oEz0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f806543-4d16-4c11-f5de-9db69e7bfccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-0.00468159  0.00374392  0.00486954 ...  0.00275183 -0.00123738\n",
            "  -0.0051892 ]\n",
            " [-0.00711169  0.00243889 -0.00271954 ... -0.00161209 -0.00322137\n",
            "  -0.00167902]\n",
            " [-0.00386294  0.00229837 -0.00203724 ... -0.00246231 -0.00096477\n",
            "   0.00092965]\n",
            " ...\n",
            " [-0.01263203 -0.0015756   0.00835914 ...  0.00726475  0.00257359\n",
            "  -0.00443831]\n",
            " [-0.00855635 -0.00109304  0.00680036 ...  0.00477306  0.00348613\n",
            "  -0.00185344]\n",
            " [-0.00775657 -0.01308013 -0.0019309  ...  0.0074421   0.00563262\n",
            "  -0.00528005]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFYW5qHXE1vP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad441698-5a8b-4877-8e3c-4cd2bdbbd187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-4.6815919e-03  3.7439226e-03  4.8695426e-03 -3.3727940e-04\n",
            "  3.2281198e-03 -2.6651658e-04  2.8959750e-03 -5.6426888e-03\n",
            "  4.8216940e-03  4.2806263e-05 -4.8583318e-03  1.2096581e-03\n",
            " -2.7677100e-03 -2.2116841e-03  5.4161469e-03 -2.1009860e-03\n",
            " -2.7926131e-03 -1.5248366e-03 -1.0337454e-02  4.6554580e-04\n",
            "  1.8401269e-03  6.2545766e-03 -8.1639979e-03  6.1668857e-04\n",
            " -8.7611042e-03  1.4593744e-03  4.5660068e-03  5.1991944e-03\n",
            "  2.0836825e-03  2.1776988e-03  4.8540961e-03  2.6007558e-03\n",
            " -2.3722029e-03  3.4262827e-03  2.7506372e-03 -3.1821977e-03\n",
            "  4.4646312e-04 -3.9158124e-03 -2.6573469e-03 -3.5640076e-03\n",
            "  1.3068549e-03 -1.0277744e-03 -8.3314744e-04 -1.1126553e-03\n",
            "  1.6631335e-03 -4.4461139e-03  3.4402222e-03 -1.9849213e-03\n",
            "  1.2478990e-03 -5.0615729e-03 -3.2595524e-03  2.2337409e-03\n",
            " -2.9287417e-05 -4.6740300e-03 -2.1397832e-03  2.3123187e-03\n",
            " -5.1874318e-05  3.0719051e-03  1.3971515e-04  4.2776340e-03\n",
            " -4.0948917e-03  1.9411938e-03  2.7518282e-03 -1.2373817e-03\n",
            " -5.1891957e-03], shape=(65,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtq6GIjGE393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7c6016a-d4d0-46b7-f43e-15b35f913a49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"BiLFDqU&O$gzIUNa:gBQzpO?f\\nMA; 3IGjz!evihxjeDKlKvpqRzINgWEQnJXGU;GWiRGJJ-:pFfOLAztXOR\\nT','3uoUfSEwU I\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gO_eKUguE6JA"
      },
      "outputs": [],
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "XBmbaH37ZqjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS6mD021E8mq"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbIqlL1KE-2w",
        "outputId": "f05b03db-63e6-4e4a-d8ae-1116a08bbae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 23s 76ms/step - loss: 2.5638\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 15s 81ms/step - loss: 1.8664\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 16s 81ms/step - loss: 1.6271\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 16s 76ms/step - loss: 1.4985\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 1.4200\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 16s 79ms/step - loss: 1.3652\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 16s 81ms/step - loss: 1.3214\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 15s 78ms/step - loss: 1.2817\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 1.2469\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 1.2108\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(data, epochs=10, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLrBicW5E--r"
      },
      "outputs": [],
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PjRa4xzFDwn"
      },
      "outputs": [],
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wivvdr96FFtf"
      },
      "outputs": [],
      "source": [
        "checkpoint_num = 10\n",
        "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "model.build(tf.TensorShape([1, None]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD5JpCWbFIQU"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9eYzldlFKTg"
      },
      "outputs": [],
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}