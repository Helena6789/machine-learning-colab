{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anp_cloned_from": {
      "notebook_id": "700375763812832",
      "revision_id": "390006458366428"
    },
    "bento_stylesheets": {
      "bento/extensions/flow/main.css": true,
      "bento/extensions/kernel_selector/main.css": true,
      "bento/extensions/kernel_ui/main.css": true,
      "bento/extensions/new_kernel/main.css": true,
      "bento/extensions/system_usage/main.css": true,
      "bento/extensions/theme/main.css": true
    },
    "disseminate_notebook_id": {
      "notebook_id": "736675493480417"
    },
    "disseminate_notebook_info": {
      "bento_version": "20191111-000215",
      "description": "BERT tutorial for OSS",
      "hide_code": false,
      "hipster_group": "",
      "kernel_build_info": {
        "deps": [
          "//aml/integrity_solutions/bento:library"
        ],
        "external_deps": []
      },
      "no_uii": true,
      "notebook_number": "173786",
      "others_can_edit": false,
      "reviewers": "",
      "revision_id": "2665524713486279",
      "tags": "",
      "tasks": "",
      "title": "Dynamic Quantization on HuggingFace BERT model"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APjloNtd60lg"
      },
      "source": [
        "# Dynamic Quantization on BERT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKWGy01rmGEW",
        "outputId": "0e841c9f-6f80-4810-e21b-dc6c36c335db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ./drive/MyDrive/ml-quant/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NKRMEHmmhSQ",
        "outputId": "d902bba2-b37e-4ec0-9521-74bcf1dfb2c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ml-quant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKRaaq8FIm3o",
        "outputId": "fbfa726b-bf77-4fb6-f7f2-e0d193f1d5cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#!pip install sklearn\n",
        "!yes y |pip uninstall transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.38.2\n",
            "Uninstalling transformers-4.38.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/transformers-cli\n",
            "    /usr/local/lib/python3.10/dist-packages/transformers-4.38.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/transformers/*\n",
            "Proceed (Y/n)?   Successfully uninstalled transformers-4.38.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "%cd transformers\n",
        "!pip install ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOr7jx_taFbP",
        "outputId": "02643ba9-0d82-4de8-afd1-20bbeb809b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 193283, done.\u001b[K\n",
            "remote: Counting objects: 100% (23830/23830), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1543/1543), done.\u001b[K\n",
            "remote: Total 193283 (delta 23335), reused 22388 (delta 22265), pack-reused 169453\u001b[K\n",
            "Receiving objects: 100% (193283/193283), 201.49 MiB | 22.79 MiB/s, done.\n",
            "Resolving deltas: 100% (137756/137756), done.\n",
            "/content/transformers\n",
            "Processing /content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0.dev0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0.dev0) (2024.2.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.40.0.dev0-py3-none-any.whl size=8802691 sha256=761fb9d5928725764547c655d0e75251aad3ce0075bfeec8001fcae670f5d65a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rc16ugf6/wheels/7c/35/80/e946b22a081210c6642e607ed65b2a5b9a4d9259695ee2caf5\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.40.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_y6e6cYE-uQ"
      },
      "source": [
        "Because we will be using the experimental parts of the PyTorch, it is recommended to install the latest version of torch and torchvision. You can find the most recent instructions on local installation [here](https://pytorch.org/get-started/locally/). For example, to install on Mac:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNNhTJCRFFSh"
      },
      "source": [
        "#!yes y | pip uninstall torch torchvision\n",
        "#!yes y | pip install --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnVXDp3zIgNq"
      },
      "source": [
        "## 1.2 Import the necessary modules\n",
        "\n",
        "In this step we import the necessary Python modules for the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQq5QmXLIgNr",
        "outputId": "70b960f0-d833-4258-fe0e-e74942005cea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "\n",
        "from argparse import Namespace\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from tqdm import tqdm\n",
        "from transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)\n",
        "from transformers import glue_compute_metrics as compute_metrics\n",
        "from transformers import glue_output_modes as output_modes\n",
        "from transformers import glue_processors as processors\n",
        "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
        "\n",
        "# Setup logging\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.WARN)\n",
        "\n",
        "logging.getLogger(\"transformers.modeling_utils\").setLevel(\n",
        "   logging.WARN)  # Reduce logging\n",
        "\n",
        "print(torch.__version__)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIHLm9FrDuIQ"
      },
      "source": [
        "We set the number of threads to compare the single thread performance between FP32 and INT8 performance. In the end of the tutorial, the user can set other number of threads by building PyTorch with right parallel backend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmhM_lshDvym",
        "outputId": "e78866b5-c38a-454c-cc82-277d3f927ddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.set_num_threads(2)\n",
        "print(torch.__config__.parallel_info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ATen/Parallel:\n",
            "\tat::get_num_threads() : 2\n",
            "\tat::get_num_interop_threads() : 2\n",
            "OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "\tomp_get_max_threads() : 2\n",
            "Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "\tmkl_get_max_threads() : 2\n",
            "Intel(R) MKL-DNN v3.3.2 (Git Hash 2dc95a2ad0841e29db8b22fbccaf3e5da7992b01)\n",
            "std::thread::hardware_concurrency() : 2\n",
            "Environment variables:\n",
            "\tOMP_NUM_THREADS : [not set]\n",
            "\tMKL_NUM_THREADS : [not set]\n",
            "ATen parallel backend: OpenMP\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySc1BfWdIgNu"
      },
      "source": [
        "## 1.4 Download the dataset\n",
        "\n",
        "Before running MRPC tasks we download the [GLUE data](https://gluebenchmark.com/tasks) by running [this script](https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e) and unpack it to some directory `glue_data/MRPC`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw6D1aSHIgNv",
        "outputId": "b11f9e47-45b5-47db-f925-7131459d7702",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !python download_glue_data.py --data_dir='glue_data' --tasks='MRPC' --test_labels=True\n",
        "!pwd\n",
        "!ls\n",
        "!wget https://gist.githubusercontent.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/raw/17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py\n",
        "!python download_glue_data.py --data_dir='glue_data' --tasks='MRPC'\n",
        "!ls glue_data/MRPC\n",
        "!mv glue_data/MRPC glue_data/mrpc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ml-quant\n",
            "--2024-03-24 22:36:12--  https://gist.githubusercontent.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/raw/17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8225 (8.0K) [text/plain]\n",
            "Saving to: ‘download_glue_data.py’\n",
            "\n",
            "download_glue_data. 100%[===================>]   8.03K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2024-03-24 22:36:12 (1.01 MB/s) - ‘download_glue_data.py’ saved [8225/8225]\n",
            "\n",
            "Processing MRPC...\n",
            "Local MRPC data not specified, downloading data from https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\n",
            "\tCompleted!\n",
            "dev_ids.tsv  dev.tsv  msr_paraphrase_test.txt  msr_paraphrase_train.txt  test.tsv  train.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y492FMyIgN0"
      },
      "source": [
        "# 2 Fine-tune the BERT model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/text-classification/run_glue.py\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/text-classification/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27l9dSWCUk1v",
        "outputId": "1978f495-7ee8-411e-c084-27bca4a5d2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "--2024-03-24 03:55:43--  https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/text-classification/run_glue.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28440 (28K) [text/plain]\n",
            "Saving to: ‘run_glue.py’\n",
            "\n",
            "run_glue.py         100%[===================>]  27.77K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-03-24 03:55:43 (15.9 MB/s) - ‘run_glue.py’ saved [28440/28440]\n",
            "\n",
            "--2024-03-24 03:55:43--  https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/text-classification/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 112 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]     112  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-24 03:55:44 (2.90 MB/s) - ‘requirements.txt’ saved [112/112]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8LuwFLKZRvn",
        "outputId": "74521179-0777-4d3c-9824-09aac8c8b081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate>=0.12.0 (from -r requirements.txt (line 1))\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=1.8.0 (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.1.99)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.2.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.20.3)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.2.1+cu121)\n",
            "Collecting evaluate (from -r requirements.txt (line 8))\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.66.2)\n",
            "Collecting xxhash (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.9.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m783.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.3->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.3->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.3->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.3->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.3->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.3->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.3->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19 (from evaluate->-r requirements.txt (line 8))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->-r requirements.txt (line 7)) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (1.16.0)\n",
            "Installing collected packages: xxhash, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, responses, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, evaluate, accelerate\n",
            "Successfully installed accelerate-0.28.0 datasets-2.18.0 dill-0.3.8 evaluate-0.4.1 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 responses-0.18.0 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKaw16X_j1rr",
        "outputId": "17b24956-ff30-413b-fa68-efb77e7b0a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--model_name_or_path openai-community/gpt2-xl \\\n",
        "!python run_glue.py \\\n",
        "    --model_name_or_path google-bert/bert-large-uncased \\\n",
        "    --task_name mrpc \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_gpu_eval_batch_size=8   \\\n",
        "    --per_gpu_train_batch_size=8   \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --save_steps 100000 \\\n",
        "    --output_dir google-bert\\\n",
        "    --overwrite_output_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YljXg9OnUqYt",
        "outputId": "80e87359-f4c5-4f7d-b9d6-0d95fd3fdd75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-24 03:59:58.127811: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-24 03:59:58.127863: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-24 03:59:58.129207: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-24 03:59:59.385050: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/24/2024 04:00:02 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/24/2024 04:00:02 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=google-bert/runs/Mar24_04-00-02_b74feb991308,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=google-bert,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=google-bert,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=100000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "https://huggingface.co/datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/88644a214f5ff54714fe06b4fe093ddb2a8b6a14231e936ad93863dfb228440f.2a68834d42622b6a5a53ccfa60ef536828f83a5e011882a000ccbdace73dd8ac.incomplete\n",
            "03/24/2024 04:00:04 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/88644a214f5ff54714fe06b4fe093ddb2a8b6a14231e936ad93863dfb228440f.2a68834d42622b6a5a53ccfa60ef536828f83a5e011882a000ccbdace73dd8ac.incomplete\n",
            "Downloading readme: 100% 35.3k/35.3k [00:00<00:00, 56.6MB/s]\n",
            "storing https://huggingface.co/datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md in cache at /root/.cache/huggingface/datasets/downloads/88644a214f5ff54714fe06b4fe093ddb2a8b6a14231e936ad93863dfb228440f.2a68834d42622b6a5a53ccfa60ef536828f83a5e011882a000ccbdace73dd8ac\n",
            "03/24/2024 04:00:04 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md in cache at /root/.cache/huggingface/datasets/downloads/88644a214f5ff54714fe06b4fe093ddb2a8b6a14231e936ad93863dfb228440f.2a68834d42622b6a5a53ccfa60ef536828f83a5e011882a000ccbdace73dd8ac\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/88644a214f5ff54714fe06b4fe093ddb2a8b6a14231e936ad93863dfb228440f.2a68834d42622b6a5a53ccfa60ef536828f83a5e011882a000ccbdace73dd8ac\n",
            "03/24/2024 04:00:04 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/88644a214f5ff54714fe06b4fe093ddb2a8b6a14231e936ad93863dfb228440f.2a68834d42622b6a5a53ccfa60ef536828f83a5e011882a000ccbdace73dd8ac\n",
            "Generating dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "03/24/2024 04:00:05 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Downloading and preparing dataset glue/mrpc to /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c...\n",
            "03/24/2024 04:00:05 - INFO - datasets.builder - Downloading and preparing dataset glue/mrpc to /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c...\n",
            "Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "03/24/2024 04:00:05 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/5a344461295233e75a33115e3fa05336895962882fba5fa045aa6e0db7e9dc1d.incomplete\n",
            "03/24/2024 04:00:05 - INFO - datasets.utils.file_utils - hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/5a344461295233e75a33115e3fa05336895962882fba5fa045aa6e0db7e9dc1d.incomplete\n",
            "Downloading data: 100% 649k/649k [00:00<00:00, 2.29MB/s]\n",
            "storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc/train-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/5a344461295233e75a33115e3fa05336895962882fba5fa045aa6e0db7e9dc1d\n",
            "03/24/2024 04:00:06 - INFO - datasets.utils.file_utils - storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc/train-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/5a344461295233e75a33115e3fa05336895962882fba5fa045aa6e0db7e9dc1d\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/5a344461295233e75a33115e3fa05336895962882fba5fa045aa6e0db7e9dc1d\n",
            "03/24/2024 04:00:06 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5a344461295233e75a33115e3fa05336895962882fba5fa045aa6e0db7e9dc1d\n",
            "hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/ee67f340dfc664a58d521653a7ba11a39d4ec168cc45843086074ac7e3b8a6e9.incomplete\n",
            "03/24/2024 04:00:06 - INFO - datasets.utils.file_utils - hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/ee67f340dfc664a58d521653a7ba11a39d4ec168cc45843086074ac7e3b8a6e9.incomplete\n",
            "Downloading data: 100% 75.7k/75.7k [00:00<00:00, 732kB/s]\n",
            "storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc/validation-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/ee67f340dfc664a58d521653a7ba11a39d4ec168cc45843086074ac7e3b8a6e9\n",
            "03/24/2024 04:00:06 - INFO - datasets.utils.file_utils - storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc/validation-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/ee67f340dfc664a58d521653a7ba11a39d4ec168cc45843086074ac7e3b8a6e9\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/ee67f340dfc664a58d521653a7ba11a39d4ec168cc45843086074ac7e3b8a6e9\n",
            "03/24/2024 04:00:06 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/ee67f340dfc664a58d521653a7ba11a39d4ec168cc45843086074ac7e3b8a6e9\n",
            "hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc/test-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/f2f810873ec60955c7329994840fbdfc4d48e14fdf4ff914eba49a6c07c77d12.incomplete\n",
            "03/24/2024 04:00:06 - INFO - datasets.utils.file_utils - hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc/test-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/f2f810873ec60955c7329994840fbdfc4d48e14fdf4ff914eba49a6c07c77d12.incomplete\n",
            "Downloading data: 100% 308k/308k [00:00<00:00, 3.09MB/s]\n",
            "storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc/test-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/f2f810873ec60955c7329994840fbdfc4d48e14fdf4ff914eba49a6c07c77d12\n",
            "03/24/2024 04:00:07 - INFO - datasets.utils.file_utils - storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc/test-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/f2f810873ec60955c7329994840fbdfc4d48e14fdf4ff914eba49a6c07c77d12\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/f2f810873ec60955c7329994840fbdfc4d48e14fdf4ff914eba49a6c07c77d12\n",
            "03/24/2024 04:00:07 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/f2f810873ec60955c7329994840fbdfc4d48e14fdf4ff914eba49a6c07c77d12\n",
            "Downloading took 0.0 min\n",
            "03/24/2024 04:00:07 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "03/24/2024 04:00:07 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "03/24/2024 04:00:07 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 100% 3668/3668 [00:00<00:00, 100297.33 examples/s]\n",
            "Generating validation split\n",
            "03/24/2024 04:00:07 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 100% 408/408 [00:00<00:00, 182575.06 examples/s]\n",
            "Generating test split\n",
            "03/24/2024 04:00:07 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 100% 1725/1725 [00:00<00:00, 364823.24 examples/s]\n",
            "All the splits matched successfully.\n",
            "03/24/2024 04:00:07 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c. Subsequent calls will reuse this data.\n",
            "03/24/2024 04:00:07 - INFO - datasets.builder - Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c. Subsequent calls will reuse this data.\n",
            "config.json: 100% 571/571 [00:00<00:00, 3.04MB/s]\n",
            "[INFO|configuration_utils.py:726] 2024-03-24 04:00:07,528 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-bert--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-03-24 04:00:07,536 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"google-bert/bert-large-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 277kB/s]\n",
            "[INFO|configuration_utils.py:726] 2024-03-24 04:00:07,715 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-bert--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-03-24 04:00:07,715 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"google-bert/bert-large-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 4.37MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 4.12MB/s]\n",
            "[INFO|tokenization_utils_base.py:2096] 2024-03-24 04:00:08,525 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--google-bert--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2096] 2024-03-24 04:00:08,525 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google-bert--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2024-03-24 04:00:08,525 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2096] 2024-03-24 04:00:08,525 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2096] 2024-03-24 04:00:08,525 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google-bert--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:726] 2024-03-24 04:00:08,525 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-bert--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-03-24 04:00:08,526 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"google-bert/bert-large-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "model.safetensors: 100% 1.34G/1.34G [00:14<00:00, 92.8MB/s]\n",
            "[INFO|modeling_utils.py:3283] 2024-03-24 04:00:23,300 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google-bert--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b/model.safetensors\n",
            "[INFO|modeling_utils.py:4014] 2024-03-24 04:00:24,657 >> Some weights of the model checkpoint at google-bert/bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:4026] 2024-03-24 04:00:24,657 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/3668 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4b66dae454115776.arrow\n",
            "03/24/2024 04:00:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4b66dae454115776.arrow\n",
            "Running tokenizer on dataset: 100% 3668/3668 [00:01<00:00, 3187.12 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/408 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-62afa266fbb9bffe.arrow\n",
            "03/24/2024 04:00:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-62afa266fbb9bffe.arrow\n",
            "Running tokenizer on dataset: 100% 408/408 [00:00<00:00, 4008.35 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1725 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-e28ffeca9730836a.arrow\n",
            "03/24/2024 04:00:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-e28ffeca9730836a.arrow\n",
            "Running tokenizer on dataset: 100% 1725/1725 [00:00<00:00, 4063.38 examples/s]\n",
            "03/24/2024 04:00:26 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [101, 1996, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 3943, 5530, 1997, 5491, 2041, 16992, 9989, 2114, 2032, 1012, 102, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 1037, 3943, 1011, 3931, 19238, 3661, 2000, 1996, 2457, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "03/24/2024 04:00:26 - INFO - __main__ - Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [101, 18178, 8661, 4584, 2551, 2005, 1996, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 2279, 4465, 1005, 1055, 4883, 2602, 1999, 2162, 1011, 7950, 18178, 2818, 17238, 1012, 102, 4584, 1999, 18178, 2818, 17238, 1005, 1055, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1010, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 4465, 1005, 1055, 4883, 2602, 1999, 1996, 2162, 1011, 25537, 2555, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "03/24/2024 04:00:26 - INFO - __main__ - Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [101, 3115, 1004, 3532, 1005, 1055, 3156, 4518, 5950, 17795, 6430, 1018, 1012, 2871, 2685, 2000, 5818, 2509, 1012, 2753, 1010, 2096, 17235, 2850, 4160, 17795, 3062, 1020, 1012, 1019, 2685, 2000, 1015, 1010, 18744, 1012, 2753, 1012, 102, 1996, 3115, 1004, 3532, 1005, 1055, 3156, 5950, 2001, 2039, 1015, 1012, 4293, 2685, 1010, 2030, 1014, 1012, 2324, 3867, 1010, 2000, 5989, 2581, 1012, 6273, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "Downloading builder script: 100% 5.75k/5.75k [00:00<00:00, 20.5MB/s]\n",
            "[WARNING|training_args.py:1902] 2024-03-24 04:00:27,849 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:1902] 2024-03-24 04:00:27,849 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:768] 2024-03-24 04:00:28,085 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1972] 2024-03-24 04:00:28,106 >> ***** Running training *****\n",
            "[INFO|trainer.py:1973] 2024-03-24 04:00:28,106 >>   Num examples = 3,668\n",
            "[INFO|trainer.py:1974] 2024-03-24 04:00:28,106 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1975] 2024-03-24 04:00:28,106 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1978] 2024-03-24 04:00:28,106 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1979] 2024-03-24 04:00:28,106 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1980] 2024-03-24 04:00:28,106 >>   Total optimization steps = 1,377\n",
            "[INFO|trainer.py:1981] 2024-03-24 04:00:28,107 >>   Number of trainable parameters = 335,143,938\n",
            "{'loss': 0.5479, 'grad_norm': 26.378345489501953, 'learning_rate': 1.2737835875090777e-05, 'epoch': 1.09}\n",
            "{'loss': 0.3245, 'grad_norm': 28.331356048583984, 'learning_rate': 5.475671750181555e-06, 'epoch': 2.18}\n",
            "100% 1377/1377 [15:23<00:00,  1.69it/s][INFO|trainer.py:2234] 2024-03-24 04:15:52,075 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 924.0747, 'train_samples_per_second': 11.908, 'train_steps_per_second': 1.49, 'train_loss': 0.3687658372268178, 'epoch': 3.0}\n",
            "100% 1377/1377 [15:24<00:00,  1.49it/s]\n",
            "[INFO|trainer.py:3206] 2024-03-24 04:15:52,184 >> Saving model checkpoint to google-bert\n",
            "[INFO|configuration_utils.py:471] 2024-03-24 04:15:52,186 >> Configuration saved in google-bert/config.json\n",
            "[INFO|modeling_utils.py:2474] 2024-03-24 04:16:02,687 >> Model weights saved in google-bert/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2514] 2024-03-24 04:16:02,688 >> tokenizer config file saved in google-bert/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2523] 2024-03-24 04:16:02,688 >> Special tokens file saved in google-bert/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.3688\n",
            "  train_runtime            = 0:15:24.07\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =     11.908\n",
            "  train_steps_per_second   =       1.49\n",
            "03/24/2024 04:16:02 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:768] 2024-03-24 04:16:02,704 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[WARNING|training_args.py:1916] 2024-03-24 04:16:02,706 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[WARNING|training_args.py:1916] 2024-03-24 04:16:02,706 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:3515] 2024-03-24 04:16:02,706 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3517] 2024-03-24 04:16:02,706 >>   Num examples = 408\n",
            "[INFO|trainer.py:3520] 2024-03-24 04:16:02,706 >>   Batch size = 8\n",
            "100% 51/51 [00:10<00:00,  4.82it/s][WARNING|training_args.py:1916] 2024-03-24 04:16:13,182 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "100% 51/51 [00:10<00:00,  4.98it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.8775\n",
            "  eval_combined_score     =     0.8952\n",
            "  eval_f1                 =     0.9129\n",
            "  eval_loss               =     0.5537\n",
            "  eval_runtime            = 0:00:10.47\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =     38.946\n",
            "  eval_steps_per_second   =      4.868\n",
            "[WARNING|training_args.py:1902] 2024-03-24 04:16:13,186 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:1916] 2024-03-24 04:16:13,186 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[WARNING|training_args.py:1902] 2024-03-24 04:16:13,187 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:1916] 2024-03-24 04:16:13,187 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "853e313d-a175-455c-e5b6-253d7742208a",
        "id": "VpBpHtjFaqXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://download.pytorch.org/tutorial/MRPC.zip\n",
        "!unzip MRPC.zip\n",
        "!mkdir google-bert-base\n",
        "!mv MRPC google-bert-base/mrpc\n",
        "!ls\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-24 22:37:10--  https://download.pytorch.org/tutorial/MRPC.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 99.86.38.72, 99.86.38.37, 99.86.38.96, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|99.86.38.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 405365618 (387M) [application/zip]\n",
            "Saving to: ‘MRPC.zip’\n",
            "\n",
            "MRPC.zip            100%[===================>] 386.59M  65.9MB/s    in 6.3s    \n",
            "\n",
            "2024-03-24 22:37:17 (61.1 MB/s) - ‘MRPC.zip’ saved [405365618/405365618]\n",
            "\n",
            "Archive:  MRPC.zip\n",
            "   creating: MRPC/\n",
            " extracting: MRPC/added_tokens.json  \n",
            "  inflating: MRPC/tokenizer_config.json  \n",
            "  inflating: MRPC/special_tokens_map.json  \n",
            "  inflating: MRPC/config.json        \n",
            "  inflating: MRPC/training_args.bin  \n",
            "  inflating: MRPC/vocab.txt          \n",
            "  inflating: MRPC/pytorch_model.bin  \n",
            "download_glue_data.py  glue_data  google-bert-base  MRPC.zip\n",
            "/content/drive/MyDrive/ml-quant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18Kuew6wtipv"
      },
      "source": [
        "## 2.1 Set global configurations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O69tatmbtnJ3"
      },
      "source": [
        "Here we set the global configurations for evaluating the fine-tuned BERT model before and after the dynamic quantization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-ZdUEGOIgN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d9c72df-e76b-47c6-9600-77c35503dd08"
      },
      "source": [
        "configs = Namespace()\n",
        "\n",
        "# The output directory for the fine-tuned model.\n",
        "configs.output_dir = \"/content/google-bert-base/\"\n",
        "# configs.output_dir = \"./MRPC/\"\n",
        "\n",
        "# The data directory for the MRPC task in the GLUE benchmark.\n",
        "configs.data_dir = \"/content/glue_data/MRPC\"\n",
        "\n",
        "# The model name or path for the pre-trained model.\n",
        "configs.model_name_or_path = \"bert-base-uncased\"\n",
        "# The maximum length of an input sequence\n",
        "configs.max_seq_length = 128\n",
        "\n",
        "# Prepare GLUE task.\n",
        "configs.task_name = \"MRPC\".lower()\n",
        "configs.processor = processors[configs.task_name]()\n",
        "configs.output_mode = output_modes[configs.task_name]\n",
        "configs.label_list = configs.processor.get_labels()\n",
        "configs.model_type = \"bert\".lower()\n",
        "configs.do_lower_case = True\n",
        "\n",
        "# Set the device, batch size, topology, and caching flags.\n",
        "configs.device = \"cpu\"\n",
        "configs.per_gpu_eval_batch_size = 8\n",
        "configs.n_gpu = 0\n",
        "configs.local_rank = -1\n",
        "configs.overwrite_cache = False\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility.\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/processors/glue.py:174: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVj1KXhOIgN4"
      },
      "source": [
        "## 2.2 Load the fine-tuned BERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO6tENlSo9Pl"
      },
      "source": [
        "We load the tokenizer and fine-tuned BERT sequence classifier model (FP32) from the `configs.output_dir`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzyVSIKYIgN5",
        "outputId": "044e9088-4344-498f-d0a8-4fa8745dd037",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    configs.output_dir, do_lower_case=configs.do_lower_case)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(configs.output_dir)\n",
        "model.to(configs.device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 1024)\n",
              "      (token_type_embeddings): Embedding(2, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEQei5wkroBH"
      },
      "source": [
        "## 2.3 Define the tokenize and evaluation function\n",
        "We reuse the tokenize and evaluation function from [HuggingFace](https://github.com/huggingface/transformers/blob/master/examples/run_glue.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJsPaJXMIgN3"
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_task_names = (\"mnli\", \"mnli-mm\") if args.task_name == \"mnli\" else (args.task_name,)\n",
        "    eval_outputs_dirs = (args.output_dir, args.output_dir + '-MM') if args.task_name == \"mnli\" else (args.output_dir,)\n",
        "\n",
        "    results = {}\n",
        "    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n",
        "        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n",
        "\n",
        "        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "            os.makedirs(eval_output_dir)\n",
        "\n",
        "        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "        # Note that DistributedSampler samples randomly\n",
        "        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "        # multi-gpu eval\n",
        "        if args.n_gpu > 1:\n",
        "            model = torch.nn.DataParallel(model)\n",
        "\n",
        "        # Eval!\n",
        "        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "        logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "        eval_loss = 0.0\n",
        "        nb_eval_steps = 0\n",
        "        preds = None\n",
        "        out_label_ids = None\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            model.eval()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                inputs = {'input_ids':      batch[0],\n",
        "                          'attention_mask': batch[1],\n",
        "                          'labels':         batch[3]}\n",
        "                if args.model_type != 'distilbert':\n",
        "                    inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n",
        "                outputs = model(**inputs)\n",
        "                tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "                eval_loss += tmp_eval_loss.mean().item()\n",
        "            nb_eval_steps += 1\n",
        "            if preds is None:\n",
        "                preds = logits.detach().cpu().numpy()\n",
        "                out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
        "            else:\n",
        "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "        if args.output_mode == \"classification\":\n",
        "            preds = np.argmax(preds, axis=1)\n",
        "        elif args.output_mode == \"regression\":\n",
        "            preds = np.squeeze(preds)\n",
        "        result = compute_metrics(eval_task, preds, out_label_ids)\n",
        "        results.update(result)\n",
        "\n",
        "        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"w\") as writer:\n",
        "            logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "            for key in sorted(result.keys()):\n",
        "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def load_and_cache_examples(args, task, tokenizer, evaluate=False):\n",
        "    if args.local_rank not in [-1, 0] and not evaluate:\n",
        "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "    processor = processors[task]()\n",
        "    output_mode = output_modes[task]\n",
        "    # Load data features from cache or dataset file\n",
        "    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format(\n",
        "        'dev' if evaluate else 'train',\n",
        "        list(filter(None, args.model_name_or_path.split('/'))).pop(),\n",
        "        str(args.max_seq_length),\n",
        "        str(task)))\n",
        "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
        "        label_list = processor.get_labels()\n",
        "        if task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta']:\n",
        "            # HACK(label indices are swapped in RoBERTa pretrained model)\n",
        "            label_list[1], label_list[2] = label_list[2], label_list[1]\n",
        "        examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n",
        "        features = convert_examples_to_features(examples,\n",
        "                                                tokenizer,\n",
        "                                                label_list=label_list,\n",
        "                                                max_length=args.max_seq_length,\n",
        "                                                output_mode=output_mode,\n",
        "                                                task=None,\n",
        "                                                #pad_on_left=bool(args.model_type in ['xlnet']),                 # pad on the left for xlnet\n",
        "                                                #pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
        "                                                #pad_token_segment_id=4 if args.model_type in ['xlnet'] else 0,\n",
        "        )\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "            torch.save(features, cached_features_file)\n",
        "\n",
        "    if args.local_rank == 0 and not evaluate:\n",
        "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
        "    if output_mode == \"classification\":\n",
        "        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "    elif output_mode == \"regression\":\n",
        "        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
        "    return dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWQ6E5z6IgN7"
      },
      "source": [
        "# 3. Apply the dynamic quantization\n",
        "\n",
        "We call `torch.quantization.quantize_dynamic` on the model to apply the dynamic quantization on the HuggingFace BERT model. Specifically,\n",
        "\n",
        "- We specify that we want the torch.nn.Linear modules in our model to be quantized;\n",
        "- We specify that we want weights to be converted to quantized int8 values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnMpkij7IgN7",
        "outputId": "9f3a4c7d-c57b-4e0f-e384-b763b6d33b79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "print(quantized_model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertForSequenceClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 1024)\n",
            "      (token_type_embeddings): Embedding(2, 1024)\n",
            "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-23): 24 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "              (key): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "              (value): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): DynamicQuantizedLinear(in_features=1024, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXAb1bHcIgN-"
      },
      "source": [
        "## 3.1 Check the model size\n",
        "Let's first check the model size. We can observe a significant reduction in model size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sTUmFJfIgN-",
        "outputId": "19cf736f-b999-4ea1-b8e7-f3656fae53d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')\n",
        "\n",
        "print_size_of_model(model)\n",
        "print_size_of_model(quantized_model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size (MB): 1340.703993\n",
            "Size (MB): 431.695749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "first_parameter = next(model.parameters())\n",
        "input_shape = first_parameter.size()\n",
        "print(input_shape)\n",
        "input_ids = torch.randint(0, 1000, (1, 128))\n",
        "#summary(model, input_shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc_qamj6Na_V",
        "outputId": "51ce8960-a600-4fd2-bacb-c14192031c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30522, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Laj0oGh4ifxu"
      },
      "source": [
        "## 3.2 Evaluate the inference accuracy and time\n",
        "\n",
        "Next, let's compare the inference time as well as the evaluation accuracy between the original FP32 model and the INT8 model after the dynamic quantization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_num_threads(1)\n",
        "print(torch.__config__.parallel_info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUHtPtxqvt6b",
        "outputId": "c7680be6-22e4-4843-d639-ff8ebec235f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ATen/Parallel:\n",
            "\tat::get_num_threads() : 1\n",
            "\tat::get_num_interop_threads() : 2\n",
            "OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "\tomp_get_max_threads() : 1\n",
            "Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "\tmkl_get_max_threads() : 1\n",
            "Intel(R) MKL-DNN v3.3.2 (Git Hash 2dc95a2ad0841e29db8b22fbccaf3e5da7992b01)\n",
            "std::thread::hardware_concurrency() : 2\n",
            "Environment variables:\n",
            "\tOMP_NUM_THREADS : [not set]\n",
            "\tMKL_NUM_THREADS : [not set]\n",
            "ATen parallel backend: OpenMP\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foe-dVxHIgOC",
        "outputId": "2763fc50-ad02-4743-f5c0-091526d3e96f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def time_model_evaluation(model, configs, tokenizer):\n",
        "    eval_start_time = time.time()\n",
        "    result = evaluate(configs, model, tokenizer, prefix=\"\")\n",
        "    eval_end_time = time.time()\n",
        "    eval_duration_time = eval_end_time - eval_start_time\n",
        "    print(result)\n",
        "    print(\"Evaluate total time (seconds): {0:.1f}\".format(eval_duration_time))\n",
        "\n",
        "# Evaluate the original FP32 BERT model\n",
        "time_model_evaluation(model, configs, tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 51/51 [06:08<00:00,  7.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'acc': 0.8774509803921569, 'f1': 0.9128919860627178, 'acc_and_f1': 0.8951714832274373}\n",
            "Evaluate total time (seconds): 368.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdMd7b5aIgOE",
        "outputId": "f4476165-bff4-4c01-d249-268ea00a3ccb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Evaluate the INT8 BERT model after the dynamic quantization\n",
        "time_model_evaluation(quantized_model, configs, tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/processors/glue.py:174: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
            "Evaluating: 100%|██████████| 51/51 [04:54<00:00,  5.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'acc': 0.8308823529411765, 'f1': 0.8761220825852782, 'acc_and_f1': 0.8535022177632274}\n",
            "Evaluate total time (seconds): 295.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "g049cwB-IgOJ"
      },
      "source": [
        "### TenaryBERT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huawei-noah/Pretrained-Language-Model.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjdGBj-OWuEn",
        "outputId": "be62965f-445d-4585-a39b-99e75c1cc7e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Pretrained-Language-Model'...\n",
            "remote: Enumerating objects: 1253, done.\u001b[K\n",
            "remote: Counting objects: 100% (280/280), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 1253 (delta 173), reused 120 (delta 119), pack-reused 973\u001b[K\n",
            "Receiving objects: 100% (1253/1253), 29.72 MiB | 13.78 MiB/s, done.\n",
            "Resolving deltas: 100% (540/540), done.\n",
            "Updating files: 100% (525/525), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ml-quant/Pretrained-Language-Model/TernaryBERT/\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV_ERu83W5R-",
        "outputId": "83031ee5-f711-45ab-cd67-d385e5561e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ml-quant/Pretrained-Language-Model/TernaryBERT\n",
            " LICENSE       quant_task_glue.py    requirements.txt\t\t\t        utils_glue.py\n",
            " main.png      quant_task_squad.py  'THIRD PARTY OPEN SOURCE SOFTWARE NOTICE'   utils_squad.py\n",
            " __pycache__   README.md\t     transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NcAg_ByXAIz",
        "outputId": "f288b57a-ad0b-413c-9a03-7a89a59a94ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3 (from -r requirements.txt (line 1))\n",
            "  Using cached boto3-1.34.69-py3-none-any.whl (139 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.11.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.18.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (9.4.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.14.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.14.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!python quant_task_glue.py \\\n",
        "            --data_dir /content/drive/MyDrive/ml-quant/glue_data \\\n",
        "            --model_dir /content/drive/MyDrive/ml-quant/google-bert-base \\\n",
        "            --task_name mrpc \\\n",
        "            --output_dir /content/drive/MyDrive/ml-quant/output \\\n",
        "            --learning_rate 2e-5 \\\n",
        "            --num_train_epochs 3 \\\n",
        "            --weight_bits 2 \\\n",
        "            --input_bits 8 \\\n",
        "            --aug_train \\\n",
        "            --pred_distill \\\n",
        "            --intermediate_distill \\\n",
        "            --save_fp_model \\\n",
        "            --save_quantized_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2TVdGOMYRce",
        "outputId": "a71ded72-86ae-4cb3-e32d-51689623bc26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ml-quant/Pretrained-Language-Model/TernaryBERT\n",
            "2024-03-24 23:06:11.043504: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-24 23:06:11.043566: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-24 23:06:11.045479: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-24 23:06:11.056607: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-24 23:06:13.110880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/24 11:06:14 PM The args: Namespace(data_dir='/content/drive/MyDrive/ml-quant/glue_data', model_dir='/content/drive/MyDrive/ml-quant/google-bert-base', teacher_model=None, student_model=None, task_name='mrpc', output_dir='/content/drive/MyDrive/ml-quant/output', learning_rate=2e-05, num_train_epochs=3.0, seed=42, aug_train=False, pred_distill=True, intermediate_distill=True, save_fp_model=True, save_quantized_model=True, weight_bits=2, input_bits=8, clip_val=2.5)\n",
            "03/24 11:06:15 PM Writing example 0 of 3668\n",
            "03/24 11:06:15 PM *** Example ***\n",
            "03/24 11:06:15 PM guid: train-1\n",
            "03/24 11:06:15 PM tokens: [CLS] am ##ro ##zi accused his brother , whom he called \" the witness \" , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only \" the witness \" , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]\n",
            "03/24 11:06:15 PM input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24 11:06:15 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24 11:06:15 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24 11:06:15 PM label: 1\n",
            "03/24 11:06:15 PM label_id: 1\n",
            "03/24 11:06:17 PM Writing example 0 of 408\n",
            "03/24 11:06:17 PM *** Example ***\n",
            "03/24 11:06:17 PM guid: dev-1\n",
            "03/24 11:06:17 PM tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] \" the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]\n",
            "03/24 11:06:17 PM input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24 11:06:17 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24 11:06:17 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/24 11:06:17 PM label: 1\n",
            "03/24 11:06:17 PM label_id: 1\n",
            "03/24 11:06:17 PM Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/24 11:06:19 PM Loading model /content/drive/MyDrive/ml-quant/google-bert-base/mrpc/pytorch_model.bin\n",
            "03/24 11:06:20 PM loading model...\n",
            "03/24 11:06:20 PM done!\n",
            "03/24 11:06:24 PM loading configuration file /content/drive/MyDrive/ml-quant/google-bert-base/mrpc/config.json\n",
            "03/24 11:06:24 PM Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"clip_val\": 2.5,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"input_bits\": 8,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pruned_heads\": {},\n",
            "  \"quantize_act\": true,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"weight_bits\": 2\n",
            "}\n",
            "\n",
            "03/24 11:06:24 PM Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"clip_val\": 2.5,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"input_bits\": 8,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pruned_heads\": {},\n",
            "  \"quantize_act\": true,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"weight_bits\": 2\n",
            "}\n",
            "\n",
            "03/24 11:06:26 PM Loading model /content/drive/MyDrive/ml-quant/google-bert-base/mrpc/pytorch_model.bin\n",
            "03/24 11:06:27 PM loading model...\n",
            "03/24 11:06:27 PM done!\n",
            "03/24 11:06:27 PM Weights of BertForSequenceClassification not initialized from pretrained model: ['bert.embeddings.word_embeddings.weight_clip_val', 'bert.encoder.layer.0.attention.self.clip_query', 'bert.encoder.layer.0.attention.self.clip_key', 'bert.encoder.layer.0.attention.self.clip_value', 'bert.encoder.layer.0.attention.self.clip_attn', 'bert.encoder.layer.0.attention.self.query.weight_clip_val', 'bert.encoder.layer.0.attention.self.query.act_clip_val', 'bert.encoder.layer.0.attention.self.key.weight_clip_val', 'bert.encoder.layer.0.attention.self.key.act_clip_val', 'bert.encoder.layer.0.attention.self.value.weight_clip_val', 'bert.encoder.layer.0.attention.self.value.act_clip_val', 'bert.encoder.layer.0.attention.output.dense.weight_clip_val', 'bert.encoder.layer.0.attention.output.dense.act_clip_val', 'bert.encoder.layer.0.intermediate.dense.weight_clip_val', 'bert.encoder.layer.0.intermediate.dense.act_clip_val', 'bert.encoder.layer.0.output.dense.weight_clip_val', 'bert.encoder.layer.0.output.dense.act_clip_val', 'bert.encoder.layer.1.attention.self.clip_query', 'bert.encoder.layer.1.attention.self.clip_key', 'bert.encoder.layer.1.attention.self.clip_value', 'bert.encoder.layer.1.attention.self.clip_attn', 'bert.encoder.layer.1.attention.self.query.weight_clip_val', 'bert.encoder.layer.1.attention.self.query.act_clip_val', 'bert.encoder.layer.1.attention.self.key.weight_clip_val', 'bert.encoder.layer.1.attention.self.key.act_clip_val', 'bert.encoder.layer.1.attention.self.value.weight_clip_val', 'bert.encoder.layer.1.attention.self.value.act_clip_val', 'bert.encoder.layer.1.attention.output.dense.weight_clip_val', 'bert.encoder.layer.1.attention.output.dense.act_clip_val', 'bert.encoder.layer.1.intermediate.dense.weight_clip_val', 'bert.encoder.layer.1.intermediate.dense.act_clip_val', 'bert.encoder.layer.1.output.dense.weight_clip_val', 'bert.encoder.layer.1.output.dense.act_clip_val', 'bert.encoder.layer.2.attention.self.clip_query', 'bert.encoder.layer.2.attention.self.clip_key', 'bert.encoder.layer.2.attention.self.clip_value', 'bert.encoder.layer.2.attention.self.clip_attn', 'bert.encoder.layer.2.attention.self.query.weight_clip_val', 'bert.encoder.layer.2.attention.self.query.act_clip_val', 'bert.encoder.layer.2.attention.self.key.weight_clip_val', 'bert.encoder.layer.2.attention.self.key.act_clip_val', 'bert.encoder.layer.2.attention.self.value.weight_clip_val', 'bert.encoder.layer.2.attention.self.value.act_clip_val', 'bert.encoder.layer.2.attention.output.dense.weight_clip_val', 'bert.encoder.layer.2.attention.output.dense.act_clip_val', 'bert.encoder.layer.2.intermediate.dense.weight_clip_val', 'bert.encoder.layer.2.intermediate.dense.act_clip_val', 'bert.encoder.layer.2.output.dense.weight_clip_val', 'bert.encoder.layer.2.output.dense.act_clip_val', 'bert.encoder.layer.3.attention.self.clip_query', 'bert.encoder.layer.3.attention.self.clip_key', 'bert.encoder.layer.3.attention.self.clip_value', 'bert.encoder.layer.3.attention.self.clip_attn', 'bert.encoder.layer.3.attention.self.query.weight_clip_val', 'bert.encoder.layer.3.attention.self.query.act_clip_val', 'bert.encoder.layer.3.attention.self.key.weight_clip_val', 'bert.encoder.layer.3.attention.self.key.act_clip_val', 'bert.encoder.layer.3.attention.self.value.weight_clip_val', 'bert.encoder.layer.3.attention.self.value.act_clip_val', 'bert.encoder.layer.3.attention.output.dense.weight_clip_val', 'bert.encoder.layer.3.attention.output.dense.act_clip_val', 'bert.encoder.layer.3.intermediate.dense.weight_clip_val', 'bert.encoder.layer.3.intermediate.dense.act_clip_val', 'bert.encoder.layer.3.output.dense.weight_clip_val', 'bert.encoder.layer.3.output.dense.act_clip_val', 'bert.encoder.layer.4.attention.self.clip_query', 'bert.encoder.layer.4.attention.self.clip_key', 'bert.encoder.layer.4.attention.self.clip_value', 'bert.encoder.layer.4.attention.self.clip_attn', 'bert.encoder.layer.4.attention.self.query.weight_clip_val', 'bert.encoder.layer.4.attention.self.query.act_clip_val', 'bert.encoder.layer.4.attention.self.key.weight_clip_val', 'bert.encoder.layer.4.attention.self.key.act_clip_val', 'bert.encoder.layer.4.attention.self.value.weight_clip_val', 'bert.encoder.layer.4.attention.self.value.act_clip_val', 'bert.encoder.layer.4.attention.output.dense.weight_clip_val', 'bert.encoder.layer.4.attention.output.dense.act_clip_val', 'bert.encoder.layer.4.intermediate.dense.weight_clip_val', 'bert.encoder.layer.4.intermediate.dense.act_clip_val', 'bert.encoder.layer.4.output.dense.weight_clip_val', 'bert.encoder.layer.4.output.dense.act_clip_val', 'bert.encoder.layer.5.attention.self.clip_query', 'bert.encoder.layer.5.attention.self.clip_key', 'bert.encoder.layer.5.attention.self.clip_value', 'bert.encoder.layer.5.attention.self.clip_attn', 'bert.encoder.layer.5.attention.self.query.weight_clip_val', 'bert.encoder.layer.5.attention.self.query.act_clip_val', 'bert.encoder.layer.5.attention.self.key.weight_clip_val', 'bert.encoder.layer.5.attention.self.key.act_clip_val', 'bert.encoder.layer.5.attention.self.value.weight_clip_val', 'bert.encoder.layer.5.attention.self.value.act_clip_val', 'bert.encoder.layer.5.attention.output.dense.weight_clip_val', 'bert.encoder.layer.5.attention.output.dense.act_clip_val', 'bert.encoder.layer.5.intermediate.dense.weight_clip_val', 'bert.encoder.layer.5.intermediate.dense.act_clip_val', 'bert.encoder.layer.5.output.dense.weight_clip_val', 'bert.encoder.layer.5.output.dense.act_clip_val', 'bert.encoder.layer.6.attention.self.clip_query', 'bert.encoder.layer.6.attention.self.clip_key', 'bert.encoder.layer.6.attention.self.clip_value', 'bert.encoder.layer.6.attention.self.clip_attn', 'bert.encoder.layer.6.attention.self.query.weight_clip_val', 'bert.encoder.layer.6.attention.self.query.act_clip_val', 'bert.encoder.layer.6.attention.self.key.weight_clip_val', 'bert.encoder.layer.6.attention.self.key.act_clip_val', 'bert.encoder.layer.6.attention.self.value.weight_clip_val', 'bert.encoder.layer.6.attention.self.value.act_clip_val', 'bert.encoder.layer.6.attention.output.dense.weight_clip_val', 'bert.encoder.layer.6.attention.output.dense.act_clip_val', 'bert.encoder.layer.6.intermediate.dense.weight_clip_val', 'bert.encoder.layer.6.intermediate.dense.act_clip_val', 'bert.encoder.layer.6.output.dense.weight_clip_val', 'bert.encoder.layer.6.output.dense.act_clip_val', 'bert.encoder.layer.7.attention.self.clip_query', 'bert.encoder.layer.7.attention.self.clip_key', 'bert.encoder.layer.7.attention.self.clip_value', 'bert.encoder.layer.7.attention.self.clip_attn', 'bert.encoder.layer.7.attention.self.query.weight_clip_val', 'bert.encoder.layer.7.attention.self.query.act_clip_val', 'bert.encoder.layer.7.attention.self.key.weight_clip_val', 'bert.encoder.layer.7.attention.self.key.act_clip_val', 'bert.encoder.layer.7.attention.self.value.weight_clip_val', 'bert.encoder.layer.7.attention.self.value.act_clip_val', 'bert.encoder.layer.7.attention.output.dense.weight_clip_val', 'bert.encoder.layer.7.attention.output.dense.act_clip_val', 'bert.encoder.layer.7.intermediate.dense.weight_clip_val', 'bert.encoder.layer.7.intermediate.dense.act_clip_val', 'bert.encoder.layer.7.output.dense.weight_clip_val', 'bert.encoder.layer.7.output.dense.act_clip_val', 'bert.encoder.layer.8.attention.self.clip_query', 'bert.encoder.layer.8.attention.self.clip_key', 'bert.encoder.layer.8.attention.self.clip_value', 'bert.encoder.layer.8.attention.self.clip_attn', 'bert.encoder.layer.8.attention.self.query.weight_clip_val', 'bert.encoder.layer.8.attention.self.query.act_clip_val', 'bert.encoder.layer.8.attention.self.key.weight_clip_val', 'bert.encoder.layer.8.attention.self.key.act_clip_val', 'bert.encoder.layer.8.attention.self.value.weight_clip_val', 'bert.encoder.layer.8.attention.self.value.act_clip_val', 'bert.encoder.layer.8.attention.output.dense.weight_clip_val', 'bert.encoder.layer.8.attention.output.dense.act_clip_val', 'bert.encoder.layer.8.intermediate.dense.weight_clip_val', 'bert.encoder.layer.8.intermediate.dense.act_clip_val', 'bert.encoder.layer.8.output.dense.weight_clip_val', 'bert.encoder.layer.8.output.dense.act_clip_val', 'bert.encoder.layer.9.attention.self.clip_query', 'bert.encoder.layer.9.attention.self.clip_key', 'bert.encoder.layer.9.attention.self.clip_value', 'bert.encoder.layer.9.attention.self.clip_attn', 'bert.encoder.layer.9.attention.self.query.weight_clip_val', 'bert.encoder.layer.9.attention.self.query.act_clip_val', 'bert.encoder.layer.9.attention.self.key.weight_clip_val', 'bert.encoder.layer.9.attention.self.key.act_clip_val', 'bert.encoder.layer.9.attention.self.value.weight_clip_val', 'bert.encoder.layer.9.attention.self.value.act_clip_val', 'bert.encoder.layer.9.attention.output.dense.weight_clip_val', 'bert.encoder.layer.9.attention.output.dense.act_clip_val', 'bert.encoder.layer.9.intermediate.dense.weight_clip_val', 'bert.encoder.layer.9.intermediate.dense.act_clip_val', 'bert.encoder.layer.9.output.dense.weight_clip_val', 'bert.encoder.layer.9.output.dense.act_clip_val', 'bert.encoder.layer.10.attention.self.clip_query', 'bert.encoder.layer.10.attention.self.clip_key', 'bert.encoder.layer.10.attention.self.clip_value', 'bert.encoder.layer.10.attention.self.clip_attn', 'bert.encoder.layer.10.attention.self.query.weight_clip_val', 'bert.encoder.layer.10.attention.self.query.act_clip_val', 'bert.encoder.layer.10.attention.self.key.weight_clip_val', 'bert.encoder.layer.10.attention.self.key.act_clip_val', 'bert.encoder.layer.10.attention.self.value.weight_clip_val', 'bert.encoder.layer.10.attention.self.value.act_clip_val', 'bert.encoder.layer.10.attention.output.dense.weight_clip_val', 'bert.encoder.layer.10.attention.output.dense.act_clip_val', 'bert.encoder.layer.10.intermediate.dense.weight_clip_val', 'bert.encoder.layer.10.intermediate.dense.act_clip_val', 'bert.encoder.layer.10.output.dense.weight_clip_val', 'bert.encoder.layer.10.output.dense.act_clip_val', 'bert.encoder.layer.11.attention.self.clip_query', 'bert.encoder.layer.11.attention.self.clip_key', 'bert.encoder.layer.11.attention.self.clip_value', 'bert.encoder.layer.11.attention.self.clip_attn', 'bert.encoder.layer.11.attention.self.query.weight_clip_val', 'bert.encoder.layer.11.attention.self.query.act_clip_val', 'bert.encoder.layer.11.attention.self.key.weight_clip_val', 'bert.encoder.layer.11.attention.self.key.act_clip_val', 'bert.encoder.layer.11.attention.self.value.weight_clip_val', 'bert.encoder.layer.11.attention.self.value.act_clip_val', 'bert.encoder.layer.11.attention.output.dense.weight_clip_val', 'bert.encoder.layer.11.attention.output.dense.act_clip_val', 'bert.encoder.layer.11.intermediate.dense.weight_clip_val', 'bert.encoder.layer.11.intermediate.dense.act_clip_val', 'bert.encoder.layer.11.output.dense.weight_clip_val', 'bert.encoder.layer.11.output.dense.act_clip_val', 'bert.pooler.dense.weight_clip_val', 'bert.pooler.dense.act_clip_val']\n",
            "03/24 11:06:27 PM ***** Running training *****\n",
            "03/24 11:06:27 PM   Num examples = 3668\n",
            "03/24 11:06:27 PM   Batch size = 32\n",
            "03/24 11:06:27 PM   Num steps = 342\n",
            "/content/drive/MyDrive/ml-quant/Pretrained-Language-Model/TernaryBERT/transformer/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
            "03/24 11:10:57 PM ***** Running evaluation *****\n",
            "03/24 11:10:57 PM   200 step of 342.0 steps\n",
            "03/24 11:11:03 PM mrpc fp32   f1/acc:0.9018932874354562/0.8602941176470589\n",
            "03/24 11:11:03 PM f1/acc:0.8859934853420196/0.8284313725490197\n",
            "03/24 11:11:03 PM ******************** Save full precision model ********************\n",
            "03/24 11:11:10 PM ******************** Save quantized model ********************\n",
            "03/24 11:14:27 PM ***** Running evaluation *****\n",
            "03/24 11:14:27 PM   341 step of 342.0 steps\n",
            "03/24 11:14:27 PM mrpc fp32   f1/acc:0.9018932874354562/0.8602941176470589\n",
            "Previous best = f1/acc:0.8859934853420196/0.8284313725490197\n",
            "03/24 11:14:33 PM mrpc fp32   f1/acc:0.9018932874354562/0.8602941176470589\n",
            "03/24 11:14:33 PM f1/acc:0.8870703764320785/0.8308823529411765\n",
            "03/24 11:14:33 PM ******************** Save full precision model ********************\n",
            "03/24 11:14:35 PM ******************** Save quantized model ********************\n",
            "03/24 11:14:40 PM Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n",
            "03/24 11:14:41 PM Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')\n",
        "\n",
        "!pwd\n",
        "original_model = BertForSequenceClassification.from_pretrained(\"/content/google-bert-base/mrpc/\")\n",
        "quanted_ternary_model = BertForSequenceClassification.from_pretrained(\"./output/mrpc/quant\")\n",
        "#print(original_model)\n",
        "print_size_of_model(original_model)\n",
        "\n",
        "for name, module in original_model.named_modules():\n",
        "   if hasattr(module,'weight_quantizer'):\n",
        "    print(module)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHtI5jxJcwvf",
        "outputId": "977673a4-5030-446e-8ce8-dff0fb1a5564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Pretrained-Language-Model/TernaryBERT\n",
            "Size (MB): 438.000505\n"
          ]
        }
      ]
    }
  ]
}